
# Sentiment Analysis on African Languages Using Multilingual Transformers

## ğŸ“š Project Overview

This project explores the effectiveness of multilingual transformer models for sentiment classification in low-resource African languages. It compares four modelsâ€”**mBERT**, **AfriBERTa**, **AfroXLMR**, and **XLM-Roberta**â€”on four languages: **Ibo**, **Nigerian-Pidgin**, **Mozambican-Portuguese**, and **Yoruba**, using the **AfriSenti dataset**.

Explainability tools like **LIME** and **SHAP** are also integrated to help interpret model predictions and evaluate fairness.

---

## ğŸ’¾ Dataset

- **Source**: [AfriSenti-SemEval Dataset (Muhammad et al., 2023)](https://github.com/afrisenti/afrisent-semeval-2023)
- **Languages Used**: Yoruba, Nigerian-Pidgin, Ibo, Mozambican-Portuguese
- **Split**: 70% Training / 30% Test
- **Labels**: `positive`, `neutral`, `negative` â†’ Mapped to `0`, `1`, `2`

---

## ğŸ§  Models Used

| Model                  | Description                          |
|------------------------|--------------------------------------|
| `bert-base-multilingual-cased` | mBERT: General multilingual transformer |
| `castorini/afriberta_base`    | AfriBERTa: Pretrained on African corpora |
| `Davlan/afro-xlmr-base`       | AfroXLMR: African-optimized XLM-R |
| `xlm-roberta-base`            | XLM-Roberta: Trained on 100+ languages |

---

## âš™ï¸ Installation & Environment

Install required libraries:

```bash
pip install transformers datasets scikit-learn lime shap wandb
```

Log in to Weights & Biases (optional):

```bash
wandb login
```

---

## ğŸš€ How to Run

1. Clone this repository and navigate to the project folder.
2. Open the notebook:

```bash
jupyter notebook Project_Code_Final.ipynb
```

3. The notebook includes:
   - Preprocessing
   - Zero-shot evaluation
   - Fine-tuning using Hugging Face Trainer
   - Evaluation (F1, AUC, Kappa, etc.)
   - Explainability (LIME, SHAP)

---

## ğŸ“ˆ Evaluation Metrics

- Accuracy  
- Precision / Recall / F1 (Macro)  
- ROC AUC (OvR)  
- Cohenâ€™s Kappa  

Baseline performance is compared with a majority-class predictor and zero-shot inference.

---

## ğŸ§ª Results Summary

- **AfriBERTa** and **AfroXLMR** consistently outperformed general-purpose models in Ibo and Yoruba.
- **Nigerian-Pidgin** was the most challenging due to code-switching and informal structures.
- **Explainability**:
  - LIME highlighted alignment of predictions with emotion-laden tokens.
  - SHAP revealed token-level attribution inconsistencies in low-resource setups.

---

## ğŸ›¡ï¸ Responsible AI

- Bias evaluation showed lower performance on informal dialects like Pidgin.
- Models pretrained on African corpora were more robust and fair.
- Transparent tracking and reproducibility were maintained using W&B and open datasets.

---

## ğŸ“Œ Limitations

- Explainability (XAI) limited to a subset of models/languages.
- Nigerian-Pidgin results still unstable.
- ALE plots not implemented.

---

## ğŸ“‚ Directory Structure

```
â”œâ”€â”€ Project_Code_Final.ipynb     # Main notebook with full pipeline
â”œâ”€â”€ Final Report.pdf             # Full technical report
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ project_code_final.py        # Full codebase used for experiments and running code
```

---

## ğŸ¤ Authors

- Niel Nortier  
- Fhulufhelo Tshivhula  
- Jano Esterhuizen  

University of Pretoria, COS760
