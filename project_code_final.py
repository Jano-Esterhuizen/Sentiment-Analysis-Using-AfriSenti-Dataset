# -*- coding: utf-8 -*-
"""Project Code Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwOMMJKQMT9GOo6Eh3Y05Qt0pW-vAQNT
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from huggingface_hub import login
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset
import wandb
#from datasets import Dataset, load_metric
from tqdm import tqdm
import numpy as np
import os
import pickle
login(token="Use own key")
wandbkey = "Use own key"
# %pip install transformers datasets wandb scikit-learn
# %pip install --upgrade transformers

"""---
# Configuration

> Within this configuration stage we are logging into Github and loading the AfriSenti Datasets.

>We are also specifing the model names to be retrieved from huggingface.
---


"""

# Label mapping (used for label -> ID and ID -> label)
label2id = {"positive": 0, "neutral": 1, "negative": 2}
id2label = {v: k for k, v in label2id.items()}

# Dataset URLs
datasets_cfg = {
    "yoruba": {
        "train": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/yor/train.tsv",
        "test": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/yor/test.tsv"
    },
    "piggin": {
        "train": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/pcm/train.tsv",
        "test": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/pcm/test.tsv"
    },
    "ibo": {
        "train": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/ibo/train.tsv",
        "test": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/ibo/test.tsv"
    },
    "portuguese": {
        "train": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/por/train.tsv",
        "test": "https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/por/test.tsv"
    }
}

# Models
model_names = {
    "mbert": "bert-base-multilingual-cased",
    "afribert": "castorini/afriberta_base",
    "afroxlmr": "Davlan/afro-xlmr-base",
    "xlmr": "xlm-roberta-base"
}

"""---
# CLEAN DATA + CONVERT TO HUGGINGFACE DATASETS
>The dataset is cleaned, lowercased and emojis are removed as they have been proven to confuse sentiments with incorrect use in normal context.

>After some preproccessing, each model and language are tokenized

---


"""

#Cleaning Function
def clean_text(text: str) -> str:
    text = re.sub(r"http[s]?://\S+", "", text)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"\bRT\b", "", text)
    text = re.sub(r'"+', '"', text)
    text = re.sub(r"[^\w\sÀ-ÿ']+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text.lower()

# Data storage
results = {}

# Clean and convert data to HuggingFace dataset
for lang, urls in datasets_cfg.items():
    print(f"\n Processing language: {lang}")

    df_train = pd.read_csv(urls["train"], sep="\t")
    df_test = pd.read_csv(urls["test"], sep="\t")

    # Clean tweets
    df_train["clean_text"] = df_train["tweet"].astype(str).apply(clean_text)
    df_test["clean_text"] = df_test["tweet"].astype(str).apply(clean_text)

    # Convert labels
    df_train["label"] = df_train["label"].map(label2id)
    df_test["label"] = df_test["label"].map(label2id)

    # Drop any rows with unmapped labels
    df_train = df_train.dropna(subset=["label"])
    df_test = df_test.dropna(subset=["label"])

    # Hugging Face Dataset format
    ds_train = Dataset.from_pandas(df_train[["clean_text", "label"]])
    ds_test = Dataset.from_pandas(df_test[["clean_text", "label"]])

    results[lang] = {"train": {}, "test": {}}

    # --- STEP 2: TOKENIZE EACH SPLIT WITH EACH MODEL ---
    for model_key, model_name in model_names.items():
        print(f"   Tokenizing with {model_key}...")

        tokenizer = AutoTokenizer.from_pretrained(model_name)

        def tokenize_fn(batch):
            return tokenizer(batch["clean_text"], padding="max_length", truncation=True, max_length=128)

        results[lang]["train"][model_key] = ds_train.map(tokenize_fn, batched=True)
        results[lang]["test"][model_key] = ds_test.map(tokenize_fn, batched=True)

# Sanity check to ensure tokenized test data is in memory
try:
    _ = results["yoruba"]["test"]["mbert"]
except KeyError:
    raise RuntimeError("❗ You must run Step 1 and Step 2 first to prepare tokenized test data.")

"""---
# ZERO-SHOT SENTIMENT ANALYSIS ON TEST SETS
>Zero Shot are trained on the various models without any fine-tuning. This is to have a baseline and see the difference on what finetuning makes.

>The models are saved and stored as trained models, and the results are also captured in an overall PKL file aswell
---


"""

import os
import pickle
from transformers import AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm

# File to save progress
SAVE_PATH = "/zero_shot_results.pkl"

# Load previous progress if exists
predictions_only = {}

if os.path.exists(SAVE_PATH):
    with open(SAVE_PATH, "rb") as f:
        predictions_only = pickle.load(f)
    print("Loaded previous prediction results.")
else:
    print("No saved progress found. Starting from scratch.")


print("\n Running Zero-Shot Sentiment Classification...\n")

for lang in datasets_cfg:
    for model_key, model_name in model_names.items():
        # Skip if already done
        if lang in predictions_only and "preds" in predictions_only[lang] and model_key in predictions_only[lang]["preds"]:
            print(f"Skipping {lang} - {model_key} (already done)")
            continue

        print(f" Evaluating {model_key} on {lang} test set...")

        # Load model
        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
        model.eval()

        # Load tokenized test data
        tokenized_test = results[lang]["test"][model_key]

        all_preds = []
        all_labels = []
        all_logits = []

        for example in tqdm(tokenized_test, desc=f"{lang}-{model_key}"):
            inputs = {
                "input_ids": torch.tensor(example["input_ids"]).unsqueeze(0),
                "attention_mask": torch.tensor(example["attention_mask"]).unsqueeze(0)
            }

            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                pred = torch.argmax(logits, dim=1).item()

            all_preds.append(pred)
            all_labels.append(example["label"])
            all_logits.append(logits.squeeze(0).numpy())

        # Store results
        predictions_only.setdefault(lang, {}).setdefault("preds", {})
        predictions_only[lang]["preds"][model_key] = {
            "predictions": all_preds,
            "labels": all_labels,
            "logits": all_logits
        }

        # Save to file
        with open(SAVE_PATH, "wb") as f:
          pickle.dump(predictions_only, f)

        print(f" Saved results for {lang} - {model_key}")
        save_dir = f"./savedmodels/zero-shot/{lang}{model_key}"
        os.makedirs(save_dir, exist_ok=True)
        model.save_pretrained(save_dir)
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        tokenizer.save_pretrained(save_dir)
        print(f" Model and tokenizer saved to {save_dir}")

        print(f" Saved results for {lang} - {model_key}")

"""---
# Zero-Shot Results
>Listing and analyzing results for each model on each language in terms of:
1. Accuracy
2. Recall
3. F1-score
4. Precision

---



"""

import pickle
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score

# Load zero-shot results
with open("/zero_shot_results.pkl", "rb") as f:
    zs_results = pickle.load(f)

print(" Zero-Shot Accuracy per model per language\n")

for lang in zs_results:
    print(f" Language: {lang}")
    for model_key in zs_results[lang]["preds"]:
        preds = zs_results[lang]["preds"][model_key]["predictions"]
        labels = zs_results[lang]["preds"][model_key]["labels"]

        acc = accuracy_score(labels, preds)
        f1 = f1_score(labels, preds, average='macro')
        recall = recall_score(labels, preds, average='macro')
        precision = precision_score(labels, preds, average='macro')
        print(f"   -{model_key:<12}  ")
        print(f"     Accuracy: {acc:.4f}")
        print(f"     Recall: {recall:.4f}")
        print(f"     F1-score: {f1:.4f}")
        print(f"     Precision: {precision:.4f}")
    print()

"""---
#ROC AUC and Cohen's Kappa for Zero-Shot
>As Part of our performance results we added ROC AUC and Cohen's Kappa
---
"""

from sklearn.metrics import roc_auc_score, cohen_kappa_score
from sklearn.preprocessing import label_binarize
import numpy as np

print(" ROC AUC and Cohen’s Kappa per model per language (Zero-Shot)\n")

for lang in zs_results:
    print(f" Language: {lang}")
    for model_key in zs_results[lang]["preds"]:
        preds = np.array(zs_results[lang]["preds"][model_key]["predictions"])
        labels = np.array(zs_results[lang]["preds"][model_key]["labels"])

        print(f"  -{model_key:<12}")

        try:
            # Convert labels to one-hot (binarize) for ROC AUC
            y_true_bin = label_binarize(labels, classes=[0, 1, 2])
            y_pred_bin = label_binarize(preds, classes=[0, 1, 2])

            roc_auc = roc_auc_score(y_true_bin, y_pred_bin, average='macro', multi_class='ovr')
            print(f"    ROC AUC (OvR Macro): {roc_auc:.4f}")
        except Exception as e:
            print(f"    ROC AUC not computed: {e}")

        # Cohen's Kappa
        try:
            kappa = cohen_kappa_score(labels, preds)
            print(f"    Cohen's Kappa:       {kappa:.4f}")
        except Exception as e:
            print(f"    Cohen's Kappa not computed: {e}")
    print()

"""---
# Fine Tuned Sentiment Analysis
The fine-tuned analysis concists of training the models to the best hyper parameters. This resulted in much better performance
---
>
"""

SAVE_PATH = "finetuned_results.pkl"
model_save_dir = "FinetunedModels"

# # Load saved progress
# if os.path.exists(SAVE_PATH):
#     with open(SAVE_PATH, "rb") as f:
#         finetuned_results = pickle.load(f)
#     print(" Loaded previous fine-tuned results.")
# else:
finetuned_results = {}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


def fine_tune_full_and_save(lang, model_key, model_name):
    if lang in finetuned_results and model_key in finetuned_results[lang]:
        print(f" Skipping {lang} - {model_key} (already done)")
        return

    print(f"\n Fine-tuning {model_key} on {lang} (full data)...")

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model.to(device)
    model.eval()
    train_ds = results[lang]["train"][model_key]
    test_ds = results[lang]["test"][model_key]

    args = TrainingArguments(
        output_dir=f"./tmp/{lang}_{model_key}",
        per_device_train_batch_size=16,
        per_device_eval_batch_size=32,
        num_train_epochs=3,
        fp16=True,
        logging_strategy="no",
        save_strategy="no",
        report_to="none"
    )

    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=test_ds)
    trainer.train()
    pred = trainer.predict(test_ds)

    print(f" Saving fine-tuned model to: {model_save_dir}")
    trainer.save_model(model_save_dir) # Saves the model's weights and configuration
    tokenizer.save_pretrained(model_save_dir) # Saves the tokenizer files

    finetuned_results.setdefault(lang, {})[model_key] = {
        "logits": pred.predictions.tolist(),
        "labels": pred.label_ids.tolist()
    }

    # Save immediately to avoid loss on timeout
    with open(SAVE_PATH, "wb") as f:
        pickle.dump(finetuned_results, f)

    print(f" Saved: {lang} - {model_key}")
    save_dir = f"./savedmodels/finetuned/{lang}{model_key}"
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f" Model and tokenizer saved to {save_dir}")

for lang in datasets_cfg:
    for model_key, model_name in model_names.items():
        fine_tune_full_and_save(lang, model_key, model_name)

"""---
# Results of Fine Tuned Sentiment Analysis
---
"""

import pickle
import numpy as np
from sklearn.metrics import accuracy_score

# Load saved fine-tuned results
with open("finetuned_results.pkl", "rb") as f:
    results = pickle.load(f)

print(" Accuracy per model per language\n")

for lang in results:
    print(f" Language: {lang}")
    for model_key in results[lang]:
        logits = np.array(results[lang][model_key]["logits"])
        labels = np.array(results[lang][model_key]["labels"])
        preds = np.argmax(logits, axis=1)

        acc = accuracy_score(labels, preds)
        print(f"   {model_key:<12} → Accuracy: {acc:.4f}")
    print()

import pickle
import numpy as np
import pandas as pd
from IPython.display import display, HTML

# Load fine-tuned results
with open("finetuned_results.pkl", "rb") as f:
    finetuned_results = pickle.load(f)

# Convert logits to predictions and build raw data rows
rows = []
for lang, models in finetuned_results.items():
    for model_name, result in models.items():
        logits = result.get("logits")
        labels = result.get("labels")
        if logits is not None and labels is not None:
            for i, (logit, label) in enumerate(zip(logits, labels)):
                pred = int(np.argmax(logit))
                rows.append({
                    "language": lang,
                    "model": model_name,
                    "index": i,
                    "true_label": label,
                    "predicted_label": pred,
                    "logit": logit
                })

# Create DataFrame
df_raw_predictions = pd.DataFrame(rows)

# Display with scrollable output
display(HTML(df_raw_predictions.to_html(max_rows=65000, max_cols=20, notebook=True, escape=False)))

import pickle
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score


print(" Accuracy, Recall, and F1-score per model per language\n")


with open("finetuned_results.pkl", "rb") as f:
    results = pickle.load(f)

for lang in results:
    print(f" Language: {lang}")
    for model_key in results[lang]:
        logits = np.array(results[lang][model_key]["logits"])
        labels = np.array(results[lang][model_key]["labels"])
        preds = np.argmax(logits, axis=1)

        acc = accuracy_score(labels, preds)
        print(f"  -{model_key:<12} ")

        recall = recall_score(labels, preds, average='macro')
        f1 = f1_score(labels, preds, average='macro')
        precision = precision_score(labels, preds, average='macro')
        print(f"    Accuracy:{acc:.4f}")
        print(f"    Recall: {recall:.4f}")
        print(f"    F1-score: {f1:.4f}")
        print(f"    Precision: {precision:.4f}")

    print()

#calculate and display the ROC AUC
from sklearn.metrics import roc_auc_score, cohen_kappa_score
from sklearn.preprocessing import label_binarize

print(" ROC AUC and Cohen’s Kappa per model per language\n")

for lang in results:
    print(f" Language: {lang}")
    for model_key in results[lang]:
        logits = np.array(results[lang][model_key]["logits"])
        labels = np.array(results[lang][model_key]["labels"])
        preds = np.argmax(logits, axis=1)

        print(f"   -{model_key:<12}")

        # ROC AUC: One-vs-Rest with macro averaging (requires binarized labels)
        try:
            y_true_bin = label_binarize(labels, classes=[0, 1, 2])
            roc_auc = roc_auc_score(y_true_bin, logits, average='macro', multi_class='ovr')
            print(f"    ROC AUC (OvR Macro): {roc_auc:.4f}")
        except Exception as e:
            print(f"    ROC AUC not computed: {e}")

        # Cohen's Kappa
        kappa = cohen_kappa_score(labels, preds)
        print(f"    Cohen's Kappa:       {kappa:.4f}")

    print()



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import (
    accuracy_score, f1_score, recall_score, precision_score,
    roc_auc_score, cohen_kappa_score
)
from sklearn.preprocessing import label_binarize

# Use existing in-memory variables: zs_results and finetuned_results

# --- Metric computation helper ---
def compute_metrics(preds, labels, logits=None):
    metrics = {
        "accuracy": accuracy_score(labels, preds),
        "recall": recall_score(labels, preds, average="macro"),
        "f1": f1_score(labels, preds, average="macro"),
        "precision": precision_score(labels, preds, average="macro", zero_division=0),
        "cohen_kappa": cohen_kappa_score(labels, preds),
        "roc_auc": None  # Set default in case computation fails
    }

    if logits is not None:
        try:
            y_true_bin = label_binarize(labels, classes=[0, 1, 2])
            metrics["roc_auc"] = roc_auc_score(y_true_bin, logits, average="macro", multi_class="ovr")
        except Exception as e:
            print(f" ROC AUC failed: {e}")
    return metrics

# --- Collect results ---
rows = []

# Zero-shot metrics
for lang in zs_results:
    for model_key in zs_results[lang]["preds"]:
        logits = np.array(zs_results[lang]["preds"][model_key]["logits"])
        preds = np.array(zs_results[lang]["preds"][model_key]["predictions"])
        labels = np.array(zs_results[lang]["preds"][model_key]["labels"])
        metrics = compute_metrics(preds, labels, logits=logits)
        metrics.update({"model": model_key, "language": lang, "type": "zero-shot"})
        rows.append(metrics)

# Fine-tuned metrics
for lang in results:
    for model_key in results[lang]:
        logits = np.array(results[lang][model_key]["logits"])
        labels = np.array(results[lang][model_key]["labels"])
        preds = np.argmax(logits, axis=1)
        metrics = compute_metrics(preds, labels, logits=logits)
        metrics.update({"model": model_key, "language": lang, "type": "fine-tuned"})
        rows.append(metrics)

# Create DataFrame of metrics
df_metrics = pd.DataFrame(rows)
metrics_to_plot = ["accuracy", "f1", "recall", "precision", "roc_auc", "cohen_kappa"]

# --- Bar plots per language ---
for lang in df_metrics["language"].unique():
    df_lang = df_metrics[df_metrics["language"] == lang]

    plt.figure(figsize=(14, 6))
    for i, metric in enumerate(metrics_to_plot, 1):
        plt.subplot(2, 3, i)
        sns.barplot(data=df_lang, x="model", y=metric, hue="type", errorbar=None)
        plt.title(f"{metric.capitalize()} - {lang}")
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
    plt.tight_layout()
    plt.suptitle(f"Performance Metrics for {lang.upper()}", y=1.03, fontsize=16)
    plt.show()

# ---- PIVOT AND COMPUTE DIFFERENCES WITH ROC AUC ----

# Ensure ROC AUC gets included
metrics_to_plot = ["accuracy", "f1", "recall", "precision", "roc_auc", "cohen_kappa"]

# Pivot the full metrics DataFrame
df_pivot = df_metrics.pivot_table(index=["language", "model"], columns="type", values=metrics_to_plot)

# Flatten multi-index columns like ("accuracy", "fine-tuned") → "accuracy_fine-tuned"
df_pivot.columns = [f"{metric}_{t}" for metric, t in df_pivot.columns]

# Compute improvement = fine-tuned - zero-shot
df_diff = pd.DataFrame(index=df_pivot.index)
for metric in metrics_to_plot:
    ft_col = f"{metric}_fine-tuned"
    zs_col = f"{metric}_zero-shot"

    if ft_col in df_pivot.columns and zs_col in df_pivot.columns:
        # Compute delta if both exist
        df_diff[metric] = df_pivot[ft_col] - df_pivot[zs_col]
    else:
        print(f" Skipping metric {metric} — missing {ft_col} or {zs_col}")
        df_diff[metric] = np.nan  # Fill with NaN if any part is missing

# Drop rows where all metrics are NaN (optional)
df_diff.dropna(how="all", inplace=True)

# ---- PLOT HEATMAP INCLUDING ROC AUC ----
plt.figure(figsize=(14, 7))
sns.heatmap(
    df_diff.round(3),
    annot=True,
    cmap="coolwarm",
    center=0,
    fmt=".3f",
    linewidths=0.5,
    linecolor="gray"
)
plt.title("Fine-Tuned vs Zero-Shot: Metric Improvements (Including ROC AUC)", fontsize=14)
plt.ylabel("Language + Model")
plt.tight_layout()
plt.show()

df_diff

"""---
# XAI
---
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install lime
# %pip install shap

from google.colab import drive
drive.mount('/content/drive')

!zip -r content.zip dataset/

import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Config
languages = ["yoruba", "piggin", "ibo", "portuguese"]
model_keys = ["mbert", "afribert", "afroxlmr", "xlmr"]

for lang in languages:
    for model_key in model_keys:
        class_names = {"positive": 0, "neutral": 1, "negative": 2}

        save_path = f"/content/savedmodels/finetuned/{lang}{model_key}"
        tokenizer = AutoTokenizer.from_pretrained(save_path, use_fast=True)
        model = AutoModelForSequenceClassification.from_pretrained(save_path)
        model.eval()

        # Sample texts
        language_samples = {
                  "yoruba": [
                      "Mo nifẹ ọja yi. O dun pupọ!",
                      "O dara, ko buru, ko dara gan-an.",
                      "Eleyi buru pupọ. Iriri ti ko dara julọ ni mo ni."
                  ],
                  "piggin": [
                      "I like dis product well well!",
                      "E just dey okay, no too bad.",
                      "Wetin be dis? Worst experience I don get."
                  ],
                  "ibo": [
                      "A hụrụ m ngwaahịa a n’anya. Ọ dị egwu!",
                      "Ọ dị mma, ọ bụghị ihe kachasị mma ma ọ bụghị ihe ọjọọ.",
                      "Nke a jọgburu onwe ya. O bu ezigbo ihe ojoo."
                  ],
                  "portuguese": [
                      "Eu adoro este produto. É incrível!",
                      "Foi razoável, nem bom nem mau.",
                      "Isto é terrível. A pior experiência de sempre."
                  ]
              }
        sample_texts = language_samples[lang]

        def predict_proba(texts):
            # SHAP may pass MaskedString or other wrapper objects — convert to raw strings
            if isinstance(texts, str):
                texts = [texts]
            elif not isinstance(texts, list):
                texts = list(texts)

            # Convert all entries to string if needed (e.g., MaskedString)
            texts = [str(t) for t in texts]

            inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            return probs.cpu().numpy()


        #  SHAP masker and explainer
        masker = shap.maskers.Text(tokenizer)
        explainer = shap.Explainer(predict_proba, masker)

        #  Compute SHAP values
        shap_values = explainer(language_samples[lang])

        # Local explanation
        shap.plots.text(shap_values[0])

        import numpy as np
        import matplotlib.pyplot as plt

        from collections import defaultdict

        # Aggregate token importance
        token_contributions = defaultdict(float)

        for val in shap_values:
            for token, contrib in zip(val.data, np.abs(val.values).sum(axis=1)):
                token_contributions[token] += contrib

        # Convert to DataFrame for easy plotting
        import pandas as pd
        df_importance = pd.DataFrame(token_contributions.items(), columns=["token", "importance"])
        df_importance = df_importance.sort_values("importance", ascending=False).head(20)

        # Plot top 20 important tokens
        import matplotlib.pyplot as plt

        plt.figure(figsize=(10, 5))
        plt.barh(df_importance["token"], df_importance["importance"])
        plt.gca().invert_yaxis()
        plt.title(f"Approximate Global Word Importance from SHAP, {lang}-{model_key}")
        plt.xlabel("Total SHAP Value (abs)")
        plt.tight_layout()
        plt.show()

import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt

#  Config
languages = ["yoruba", "piggin", "ibo", "portuguese"]  # or whatever you support
model_keys = ["mbert", "afribert", "afroxlmr", "xlmr"]
class_names = ["negative", "neutral", "positive"]
for lang in languages:
    for model_key in model_keys:

        #  Load saved model + tokenizer
        model_path = f"/content/savedmodels/finetuned/{lang}{model_key}"
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.eval()

        #  Sample texts to explain
        # Sample texts per language
        language_samples = {
            "yoruba": [
                "Mo nifẹ ọja yi. O dun pupọ!",
                "O dara, ko buru, ko dara gan-an.",
                "Eleyi buru pupọ. Iriri ti ko dara julọ ni mo ni."
            ],
            "piggin": [
                "I like dis product well well!",
                "E just dey okay, no too bad.",
                "Wetin be dis? Worst experience I don get."
            ],
            "ibo": [
                "A hụrụ m ngwaahịa a n’anya. Ọ dị egwu!",
                "Ọ dị mma, ọ bụghị ihe kachasị mma ma ọ bụghị ihe ọjọọ.",
                "Nke a jọgburu onwe ya. O bu ezigbo ihe ojoo."
            ],
            "portuguese": [
                "Eu adoro este produto. É incrível!",
                "Foi razoável, nem bom nem mau.",
                "Isto é terrível. A pior experiência de sempre."
            ]
        }
        sample_texts = language_samples[lang]


        #  Prediction function for LIME
        def predict_proba(texts):
            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            return probs.cpu().numpy()

        #  Create LIME explainer
        explainer = LimeTextExplainer(class_names=class_names)

        #  Generate and display LIME explanations
        #  Generate and display LIME explanations
        for i, text in enumerate(sample_texts):
            pred_label = np.argmax(predict_proba([text])[0])
            print(f"\n LIME Explanation — Language: {lang.upper()}, Model: {model_key}, Sample {i+1}:")
            print(f"Text: {text}")
            exp = explainer.explain_instance(text, predict_proba, num_features=10, labels=[pred_label])
            exp.show_in_notebook(text=True, labels=(pred_label,))

# Define sentiment samples for each language
sample_texts_by_sentiment = {
    "positive": {
        "yoruba": "Mo nifẹ ọja yi. O dun pupọ!",
        "piggin": "I like dis product well well!",
        "ibo": "A hụrụ m ngwaahịa a n’anya. Ọ dị egwu!",
        "portuguese": "Eu adoro este produto. É incrível!"
    },
    "neutral": {
        "yoruba": "O dara, ko buru, ko dara gan-an.",
        "piggin": "E just dey okay, no too bad.",
        "ibo": "Ọ dị mma, ọ bụghị ihe kachasị mma ma ọ bụghị ihe ọjọọ.",
        "portuguese": "Foi razoável, nem bom nem mau."
    },
    "negative": {
        "yoruba": "Eleyi buru pupọ. Iriri ti ko dara julọ ni mo ni.",
        "piggin": "Wetin be dis? Worst experience I don get.",
        "ibo": "Nke a jọgburu onwe ya. O bu ezigbo ihe ojoo.",
        "portuguese": "Isto é terrível. A pior experiência de sempre."
    }
}

class_names = ["negative", "neutral", "positive"]

# Define your models
languages = ["yoruba", "piggin", "ibo", "portuguese"]
model_keys = ["mbert", "afribert", "afroxlmr", "xlmr"]

for model_key in model_keys:
    for lang in languages:
        print(f"\n LIME for {model_key.upper()} on {lang.upper()}")

        model_path = f"/content/savedmodels/finetuned/{lang}{model_key}"
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.eval()

        # Prediction function
        def predict_proba(texts):
            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            return probs.cpu().numpy()

        # LIME Explainer
        explainer = LimeTextExplainer(class_names=class_names)

        # Loop through each sentiment and explain
        for sentiment, texts_by_lang in sample_texts_by_sentiment.items():
            text = texts_by_lang[lang]
            pred_label = np.argmax(predict_proba([text])[0])
            exp = explainer.explain_instance(text, predict_proba, num_features=10, labels=[pred_label])

            # Plot explanation
            fig = exp.as_pyplot_figure(label=pred_label)
            plt.title(f"{model_key.upper()} - {lang.upper()} - {sentiment.upper()}")
            plt.tight_layout()
            plt.show()

# Config
languages = ["yoruba", "piggin", "ibo", "portuguese"]
model_keys = ["mbert", "afribert", "afroxlmr", "xlmr"]
class_names = ["negative", "neutral", "positive"]

# Representative positive text sample per language
sample_texts = {
    "yoruba": "Mo nifẹ ọja yi. O dun pupọ!",
    "piggin": "I like dis product well well!",
    "ibo": "A hụrụ m ngwaahịa a n’anya. Ọ dị egwu!",
    "portuguese": "Eu adoro este produto. É incrível!"
}

# Loop over each model to generate 1 figure with 4 subplots (one per language)
for model_key in model_keys:
    print(f"\nGenerating LIME explanations for model: {model_key.upper()}")

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, lang in enumerate(languages):
        # Load model & tokenizer
        model_path = f"/content/savedmodels/finetuned/{lang}{model_key}"
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.eval()

        # Define prediction function for LIME
        def predict_proba(texts):
            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            return probs.cpu().numpy()

        # LIME explanation
        explainer = LimeTextExplainer(class_names=class_names)
        text = sample_texts[lang]
        pred_label = np.argmax(predict_proba([text])[0])
        exp = explainer.explain_instance(text, predict_proba, num_features=10, labels=[pred_label])

        # Extract top words and weights
        exp_list = exp.as_list(label=pred_label)
        words, weights = zip(*exp_list)
        axes[i].barh(words, weights, color="skyblue")
        axes[i].set_title(f"{lang.upper()} — {class_names[pred_label].capitalize()}")
        axes[i].invert_yaxis()

    plt.suptitle(f"LIME Explanation — Model: {model_key.upper()}", fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()